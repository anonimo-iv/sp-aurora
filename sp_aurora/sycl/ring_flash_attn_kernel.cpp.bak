#include "flash_attn_kernel.h"
#include "utils.h"
#include <cstring>

namespace flash_attn {

// Ring flash attention data structure
struct RingFlashAttnData {
    float* q_local;       // Local query chunk
    float* k_buffer;      // Ring buffer for K
    float* v_buffer;      // Ring buffer for V
    float* out_accum;     // Accumulated output
    float* lse_accum;     // Accumulated LSE
    float* m_accum;       // Running max values
    float* l_accum;       // Running sum values
    int local_seq_len;    // Length of local sequence chunk
    int rank;             // Process rank
    int world_size;       // Total number of processes
};

// Ring-aware kernel that processes local Q against ring-communicated K/V
template<int BLOCK_M, int BLOCK_N, int HEAD_DIM>
class RingFlashAttnKernel {
public:
    RingFlashAttnKernel(const RingFlashAttnData& data,
                        const FlashAttnConfig& cfg,
                        int ring_step)
        : ring_data(data), config(cfg), current_step(ring_step) {}

    void operator()(nd_item<2> item) const {
        const int tid = item.get_local_id(1);
        const int batch_idx = item.get_group(0);
        const int head_idx = item.get_group(1);
        
        // Shared memory
        extern __shared__ float shared_mem[];
        float* q_smem = shared_mem;
        float* k_smem = q_smem + BLOCK_M * HEAD_DIM;
        float* v_smem = k_smem + BLOCK_N * HEAD_DIM;
        float* s_smem = v_smem + BLOCK_N * HEAD_DIM;
        
        // Calculate which K/V chunk we're processing in this ring step
        const int source_rank = (ring_data.rank - current_step + ring_data.world_size) % ring_data.world_size;
        const int k_seq_offset = source_rank * ring_data.local_seq_len;
        
        // Base pointers
        const int seq_offset = batch_idx * config.num_heads + head_idx;
        const float* q_base = ring_data.q_local + seq_offset * ring_data.local_seq_len * config.head_dim;
        const float* k_base = ring_data.k_buffer + seq_offset * ring_data.local_seq_len * config.head_dim;
        const float* v_base = ring_data.v_buffer + seq_offset * ring_data.local_seq_len * config.head_dim;
        float* out_base = ring_data.out_accum + seq_offset * ring_data.local_seq_len * config.head_dim;
        float* m_base = ring_data.m_accum + seq_offset * ring_data.local_seq_len;
        float* l_base = ring_data.l_accum + seq_offset * ring_data.local_seq_len;
        
        // Process blocks of Q against the current K/V buffer
        const int num_blocks_m = divUp(ring_data.local_seq_len, BLOCK_M);
        const int num_blocks_n = divUp(ring_data.local_seq_len, BLOCK_N);
        
        for (int block_m = 0; block_m < num_blocks_m; block_m++) {
            const int m_start = block_m * BLOCK_M;
            const int m_end = min(m_start + BLOCK_M, ring_data.local_seq_len);
            const int actual_block_m = m_end - m_start;
            
            // Load Q block
            loadTile<float, BLOCK_M>(
                q_base + m_start * config.head_dim,
                q_smem,
                0, 0,
                actual_block_m, config.head_dim,
                config.head_dim,
                item
            );
            
            // Load current max and sum values
            float m_prev[BLOCK_M];
            float l_prev[BLOCK_M];
            if (tid < actual_block_m) {
                m_prev[tid] = m_base[m_start + tid];
                l_prev[tid] = l_base[m_start + tid];
            }
            
            item.barrier();
            
            // Process K/V blocks
            for (int block_n = 0; block_n < num_blocks_n; block_n++) {
                const int n_start = block_n * BLOCK_N;
                const int n_end = min(n_start + BLOCK_N, ring_data.local_seq_len);
                const int actual_block_n = n_end - n_start;
                
                // Load K and V blocks
                loadTile<float, BLOCK_N>(
                    k_base + n_start * config.head_dim,
                    k_smem,
                    0, 0,
                    actual_block_n, config.head_dim,
                    config.head_dim,
                    item
                );
                
                loadTile<float, BLOCK_N>(
                    v_base + n_start * config.head_dim,
                    v_smem,
                    0, 0,
                    actual_block_n, config.head_dim,
                    config.head_dim,
                    item
                );
                
                item.barrier();
                
                // Compute attention scores for assigned rows
                for (int m = tid; m < actual_block_m; m += item.get_local_range(1)) {
                    float m_curr = -INFINITY;
                    
                    // Compute QK^T for this row
                    for (int n = 0; n < actual_block_n; n++) {
                        float score = 0.0f;
                        
                        #pragma unroll
                        for (int d = 0; d < HEAD_DIM; d++) {
                            score += q_smem[m * HEAD_DIM + d] * k_smem[n * HEAD_DIM + d];
                        }
                        
                        score *= config.softmax_scale;
                        
                        // Apply causal mask considering global positions
                        if (config.is_causal) {
                            int global_m = ring_data.rank * ring_data.local_seq_len + m_start + m;
                            int global_n = k_seq_offset + n_start + n;
                            if (global_n > global_m) {
                                score = -INFINITY;
                            }
                        }
                        
                        s_smem[m * BLOCK_N + n] = score;
                        m_curr = max(m_curr, score);
                    }
                    
                    // Update running max
                    float m_new = max(m_prev[m], m_curr);
                    float l_curr = 0.0f;
                    
                    // Compute exp and sum
                    for (int n = 0; n < actual_block_n; n++) {
                        float exp_score = exp(s_smem[m * BLOCK_N + n] - m_new);
                        s_smem[m * BLOCK_N + n] = exp_score;
                        l_curr += exp_score;
                    }
                    
                    // Update running sum with rescaling
                    float l_new = exp(m_prev[m] - m_new) * l_prev[m] + l_curr;
                    
                    // Load current output accumulator
                    float out_accum[HEAD_DIM];
                    #pragma unroll
                    for (int d = 0; d < HEAD_DIM; d++) {
                        out_accum[d] = out_base[(m_start + m) * config.head_dim + d];
                        // Rescale previous accumulation
                        out_accum[d] *= exp(m_prev[m] - m_new);
                    }
                    
                    // Accumulate new weighted values
                    for (int n = 0; n < actual_block_n; n++) {
                        float weight = s_smem[m * BLOCK_N + n];
                        #pragma unroll
                        for (int d = 0; d < HEAD_DIM; d++) {
                            out_accum[d] += weight * v_smem[n * HEAD_DIM + d];
                        }
                    }
                    
                    // Write back updated values
                    #pragma unroll
                    for (int d = 0; d < HEAD_DIM; d++) {
                        out_base[(m_start + m) * config.head_dim + d] = out_accum[d];
                    }
                    
                    // Update max and sum
                    m_base[m_start + m] = m_new;
                    l_base[m_start + m] = l_new;
                }
                
                item.barrier();
            }
        }
    }

private:
    RingFlashAttnData ring_data;
    FlashAttnConfig config;
    int current_step;
};

// Host function for ring flash attention forward pass
FlashAttnOutput ring_flash_attn_forward_sycl(
    sycl::queue& q,
    const float* q_local,      // Local Q chunk
    const float* k_local,      // Local K chunk  
    const float* v_local,      // Local V chunk
    float* workspace,          // Workspace for ring communication
    const FlashAttnConfig& config,
    int rank,
    int world_size
) {
    const int local_seq_len = config.seq_len_q / world_size;
    const size_t chunk_size = config.batch_size * config.num_heads * 
                              local_seq_len * config.head_dim;
    const size_t lse_chunk_size = config.batch_size * config.num_heads * local_seq_len;
    
    // Allocate device memory for accumulators and buffers
    float* d_out_accum = sycl::malloc_device<float>(chunk_size, q);
    float* d_m_accum = sycl::malloc_device<float>(lse_chunk_size, q);
    float* d_l_accum = sycl::malloc_device<float>(lse_chunk_size, q);
    float* d_k_buffer = sycl::malloc_device<float>(chunk_size, q);
    float* d_v_buffer = sycl::malloc_device<float>(chunk_size, q);
    
    // Initialize accumulators
    q.memset(d_out_accum, 0, chunk_size * sizeof(float));
    q.fill(d_m_accum, -INFINITY, lse_chunk_size);
    q.fill(d_l_accum, 0.0f, lse_chunk_size);
    
    // Copy local K/V to buffers
    q.memcpy(d_k_buffer, k_local, chunk_size * sizeof(float));
    q.memcpy(d_v_buffer, v_local, chunk_size * sizeof(float));
    q.wait();
    
    // Ring communication setup
    RingFlashAttnData ring_data = {
        const_cast<float*>(q_local),
        d_k_buffer,
        d_v_buffer,
        d_out_accum,
        nullptr,  // LSE will be computed from m and l
        d_m_accum,
        d_l_accum,
        local_seq_len,
        rank,
        world_size
    };
    
    // Kernel configuration
    const int BLOCK_M = 64;
    const int BLOCK_N = 64;
    const int THREADS = 256;
    
    sycl::range<2> global_range(config.batch_size, config.num_heads);
    sycl::range<2> local_range(1, THREADS);
    
    const size_t shmem_size = sizeof(float) * (
        BLOCK_M * config.head_dim +  // Q tile
        BLOCK_N * config.head_dim +  // K tile  
        BLOCK_N * config.head_dim +  // V tile
        BLOCK_M * BLOCK_N            // Attention scores
    );
    
    // Ring communication loop
    for (int step = 0; step < world_size; step++) {
        // Launch kernel for current step
        q.submit([&](sycl::handler& h) {
            sycl::local_accessor<float, 1> local_mem(sycl::range<1>(shmem_size / sizeof(float)), h);
            
            h.parallel_for(
                sycl::nd_range<2>(global_range * local_range, local_range),
                [=](sycl::nd_item<2> item) {
                    if (config.head_dim == 64) {
                        RingFlashAttnKernel<BLOCK_M, BLOCK_N, 64> kernel(
                            ring_data, config, step
                        );
                        kernel(item);
                    } else if (config.head_dim == 128) {
                        RingFlashAttnKernel<BLOCK_M, BLOCK_N, 128> kernel(
                            ring_data, config, step
                        );
                        kernel(item);
                    }
                }
            );
        });
        
        // Ring exchange K/V buffers (placeholder - needs MPI/oneCCL integration)
        // In practice, this would use MPI_Sendrecv or oneCCL alltoall
        // to rotate K/V chunks around the ring
        
        q.wait();
    }
    
    // Final normalization and LSE computation
    float* d_lse = sycl::malloc_device<float>(lse_chunk_size, q);
    
    q.parallel_for(sycl::range<1>(lse_chunk_size), [=](sycl::id<1> idx) {
        const int i = idx[0];
        const float inv_sum = 1.0f / d_l_accum[i];
        
        // Normalize output
        const int out_offset = i * config.head_dim;
        for (int d = 0; d < config.head_dim; d++) {
            d_out_accum[out_offset + d] *= inv_sum;
        }
        
        // Compute LSE
        d_lse[i] = d_m_accum[i] + sycl::log(d_l_accum[i]);
    });
    
    q.wait();
    
    // Cleanup intermediate buffers
    sycl::free(d_k_buffer, q);
    sycl::free(d_v_buffer, q);
    sycl::free(d_m_accum, q);
    sycl::free(d_l_accum, q);
    
    return {d_out_accum, d_lse, nullptr};
}

}  // namespace flash_attn